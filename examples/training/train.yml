########################################################################################################################
#                                                                                                                      #
#                                           PlasmaNet -- Configuration file                                            #
#                                                                                                                      #
#                                      Guillaume Bogopolsky, CERFACS, 11.03.2020                                       #
#                                                                                                                      #
########################################################################################################################

# Contains the parameters for the model (training, etc.)

# To import the parameters, use
#     import yaml
#     with open('config.yml', 'r') as yaml_stream:
#         config = yaml.safe_load(yaml_stream)

# The train routine will automatically load the required `type` of each section with the given `args` by using the
# `config.init_obj` method.


name: 'config_4/random_8'                   # Experience name
n_gpu: 1                                    # Number of GPUs to use

globals:                                    # Domain sizes and others, used to compute global attributes
    nnx: 101                                # Number of points in x direction
    nny: 101                                # Number of points in y direction
    xmin: 0.0
    xmax: 1.0e-2
    ymin: 0.0
    ymax: 1.0e-2
    coord: 'cart'                           # Coordinates system ('cart' or 'cyl')
    verbosity: 2                            # Output verbosity level

arch:
    db_file: 'unets_ks3_rf100.yml'          # Database file
    name: 'UNet5'                           # Name of the network in the database file
    args:
        input_res: 101                      # Required argument to define the input res of the network

data_loader:
    type: 'PoissonDataLoader'               # Class name of the wanted dataloader
    data_channels: 1                        # Number of data channels
    pipe_config: True                       # Does the dataloader requires the input file?
    args:
        data_dir: '/scratch/cfd/cheng/DL/datasets/train/101x101/random_8'   # Dataset path
        batch_size: 64
        shuffle: True                       # Do we randomly reorder the dataset?
        validation_split: 0.2               # if float, fraction of the full dataset, if int, length of validation portion
        num_workers: 4                      # Number of threads reading the dataset (useful for big datasets)
        normalize: 'analytical'             # Normalization of the dataset (max, physical or no)
        alpha: 0.1                          # Coefficient for analytical normalization
        scaling_factor: 1.0e+6              # Supplementary scaling factor for physical_rhs and potential

initializer: 'off'                          # Possible Pytorch initializer, cf. PyTorch doc

optimizer:
    type: 'Adam'
    args:
        lr: 4.e-4                           # Learning rate
        weight_decay: 0
        amsgrad: False                      # Use AMSGrad variant from paper 'On the convergence of Adam and Beyond'

loss:
    type: 'ComposedLoss'
    pipe_config: True                       # Object initialization require config as first argument
    args:
        loss_list:                          # List of the losses to compose if ComposedLoss is used
            # - InsideLoss
            - DirichletBoundaryLoss
            - LaplacianLoss
            # - EnergyLoss
            # - LongTermLaplacianLoss
        inside_weight: 0.0                  # Weighting of the loss inside the domain, excluding boundaries (float)
        bound_weight: 1.0                   # Weighting of the loss on the boundaries (float)
        elec_weight: 0.                     # Weighting of the loss on the electric field (float)
        lapl_weight: 2.0e+7                 # Weighting of the loss on the Laplacian (float)
        energy_weight: 0.0                  # Weighting of the loss on the energy functional (float)
        lt_weight: 0.0                      # Weighting of the lt loss, whoch only has a Laplacian term
        ltloss_num_procs: 64                # Number of processes for multiprocessing long term loss evaluation

metrics:
    - 'residual'
    - 'inf_norm'
    - 'Eresidual'
    - 'Einf_norm'

lr_scheduler:
    type: 'ReduceLROnPlateau'               # Scheduler type, cf. PyTorch doc
    plateau_metric: loss                    # Monitoring metric for ReduceLROnPlateau step()
    args:
        mode: 'min'
        factor: 0.9                         # Amount scheduler reduces LR by (float)
        patience: 50                        # Number of epochs the scheduler waits before reducing LR (int)
        threshold: 3.e-4                    # Relative improvement the scheduler must see (float)
        threshold_mode: 'rel'
        verbose: False

trainer:
    epochs: 20                              # Number of epochs
    save_dir: 'debug/'                      # Output directory
    save_period: 20                         # Output period
    plot_period: 20                         # Period to send plots to TensorBoard
    verbosity: 2
    monitor: min val_loss                   # Monitor best model ('method metric', method={min, max}, metric exists)
    early_stop: 200                         # Training is stopped if model performance does not increase for n epochs
    tensorboard: true                       # Save TensorBoard log
    histograms: false                       # Save weights and bias histograms (turned off to increase TensorBoard perf)
